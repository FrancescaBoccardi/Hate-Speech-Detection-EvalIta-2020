# -*- coding: utf-8 -*-
"""NLP_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-77F04g3d3InB8onvnTlx1kv4IHSgs-J

# NLP project - HaDeSpee Evalita 2020 challenge

Authors:
* Francesca Boccardi, francesca.boccardi@studio.unibo.it
* Luigi Podda, luigi.podda@studio.unibo.it

**Summary**: The purpose of this work is to address the
Hate Speech detection and the Stereotype detection tasks, towards a given people target, on the HaSpeeDe dataset of EvalIta, by experimenting
several combinations of word representation methods and architectures.

The EvalIta 2020 challenge is described at the following [link](http://www.di.unito.it/~tutreeb/haspeede-evalita20/index.html).

# Install & imports

In this section are installed and imported all the useful libraries and packages.
"""

# Commented out IPython magic to ensure Python compatibility.
%%capture
!pip install --upgrade numpy
!pip install compress-fasttext
!pip install transformers
!pip install datasets
!pip install evaluate
!pip install autocorrect
!pip install pattern

"""After installing packages, it is necessary to restart the colab runtime to correctly import `compress-fasttext`."""

try:
  import compress_fasttext
except (ValueError):
  ## code to install gem
  print('Stopping RUNTIME. Colaboratory will restart automatically. Please run again.')
  exit()

import pandas as pd
import numpy as np
from datasets import Dataset
import evaluate
import re
import matplotlib.pyplot as plt
import seaborn as sns
import collections

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import precision_recall_fscore_support as prfs

import keras
from keras_preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, Embedding, GlobalMaxPool1D, Dropout
from keras.optimizers import Adam
from keras import backend as K

import tensorflow
from tensorflow.compat import v1

import pattern
from pattern.text.it import parse

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import TweetTokenizer

from autocorrect import Speller

import compress_fasttext

import itertools

import random
import torch
import os

from transformers import AutoModel, AutoTokenizer, DistilBertForSequenceClassification, AutoConfig
from transformers import pipeline
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from transformers import DataCollatorWithPadding

from google.colab import drive
drive.mount('/content/drive')

"""The `set_reproducibility` function allows to make data pipeline reproducible and to train and evaluate the models using multiple and fixed seeds."""

def set_reproducibility(seed):

  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  torch.backends.cudnn.deterministic = True
  torch.backends.cudnn.benchmark = False
  tensorflow.random.set_seed(seed)
  tensorflow.keras.utils.set_random_seed(seed)
  os.environ['TF_DETERMINISTIC_OPS'] = '1'
  os.environ["PYTHONHASHSEED"] = str(seed)

"""# Data

## Data download

The EvalIta 2020 dataset is available for download at the following [link](https://github.com/msang/haspeede/tree/master/2020).  
To extract the data from the compressed folder the needed password
is *zNw3tCszKWcpDahq*.
"""

df_train = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Hate speech detection dataset/haspeede2_dev/haspeede2_dev_taskAB.tsv')
df_test_tweets = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Hate speech detection dataset/haspeede2_reference/haspeede2_reference_taskAB-tweets.tsv', header = None)
df_test_news = pd.read_table('/content/drive/MyDrive/Colab Notebooks/Hate speech detection dataset/haspeede2_reference/haspeede2_reference_taskAB-news.tsv')

df_train.columns = ['id', 'text', 'hate_speech', 'stereotype']
df_test_news.columns = ['id', 'text', 'hate_speech', 'stereotype']
df_test_tweets.columns = ['id', 'text', 'hate_speech', 'stereotype']

"""Let's have a look at the data."""

df_train

print('df_train:', df_train.shape)
print('df_test_news:', df_test_news.shape)
print('df_test_tweets:', df_test_tweets.shape)

"""## Data inspection

In this section the data are inspected by printing some samples of hate speech and stereotype language, for both tweets and news.
"""

print("Tweet without hate speech and without stereotypes:")
print(df_test_tweets[(df_test_tweets['hate_speech'] == 0) & (df_test_tweets['stereotype'] == 0)]['text'].reset_index(drop=True)[0])

print("\nTweet without hate speech and with stereotypes:")
print(df_test_tweets[(df_test_tweets['hate_speech'] == 0) & (df_test_tweets['stereotype'] == 1)]['text'].reset_index(drop=True)[0])

print("\nTweet with hate speech and without stereotypes:")
print(df_test_tweets[(df_test_tweets['hate_speech'] == 1) & (df_test_tweets['stereotype'] == 0)]['text'].reset_index(drop=True)[0])

print("\nTweet with hate speech and with stereotypes:")
print(df_test_tweets[(df_test_tweets['hate_speech'] == 1) & (df_test_tweets['stereotype'] == 1)]['text'].reset_index(drop=True)[0])

print("News without hate speech and without stereotypes:")
print(df_test_news[(df_test_news['hate_speech'] == 0) & (df_test_news['stereotype'] == 0)]['text'].reset_index(drop=True)[0])

print("\nNews without hate speech and with stereotypes:")
print(df_test_news[(df_test_news['hate_speech'] == 0) & (df_test_news['stereotype'] == 1)]['text'].reset_index(drop=True)[0])

print("\nNews with hate speech and without stereotypes:")
print(df_test_news[(df_test_news['hate_speech'] == 1) & (df_test_news['stereotype'] == 0)]['text'].reset_index(drop=True)[0])

print("\nNews with hate speech and with stereotypes:")
print(df_test_news[(df_test_news['hate_speech'] == 1) & (df_test_news['stereotype'] == 1)]['text'].reset_index(drop=True)[0])

"""In order to better understand the content of tweets and news, a text analysis is performed."""

def text_analysis(df):

  user = []
  hashtag = []
  url = []
  emoticon = []

  for sentence in df['text']:
    user = user + re.findall('@[A-Za-z0-9_]+', sentence)
    hashtag = hashtag + re.findall('#[A-Za-z0-9_]\w*', sentence)
    url = url + re.findall('\sURL', sentence)
    emoticon = emoticon + re.findall(r"[^\w\s,;.:!?%&€#/()@'\"“”\+-_|‘’…»«–—\*`$~°„]", sentence)

  return user, hashtag, url, emoticon

### Train ###

user_train, hashtag_train, url_train, emoticon_train =  text_analysis(df_train)

print("\nThe total number of HASHTAGS appearing in the train tweets is", len(hashtag_train))
print("The number of unique HASHTAGS appearing in the train tweets is", len(set(hashtag_train)))
print("\nUnique HASHTAGS:")
print(set(hashtag_train),'\n')
print('-'*1000)

print("\nThe total number of EMOTICONS appearing in the train tweets is", len(emoticon_train))
print("The number of unique EMOTICONS appearing in the train tweets is", len(set(emoticon_train)))
print("\nUnique EMOTICONS:")
print(set(emoticon_train),'\n')
print('-'*1000)

print("\nThe total number of USERNAMES appearing in the train tweets is", len(user_train))
print("The number of unique USERNAMES appearing in the train tweets is", len(set(user_train)))
print("\nUnique USERNAMES:")
print(set(user_train), '\n')
print('-'*1000)

print("\nThe total number of URLs appearing in the train tweets is", len(url_train))
print("The number of unique URLs appearing in the train tweets is", len(set(url_train)))
print("\nUnique URLs:")
print(set(url_train))

### Test tweets ###

user_test_tweets, hashtag_test_tweets, url_test_tweets, emoticon_test_tweets =  text_analysis(df_test_tweets)

print("\nThe total number of HASHTAGS appearing in the test tweets is", len(hashtag_test_tweets))
print("The number of unique HASHTAGS appearing in the test tweets is", len(set(hashtag_test_tweets)))
print("\nUnique HASHTAGS:")
print(set(hashtag_test_tweets),'\n')
print('-'*1000)

print("\nThe total number of EMOTICONS appearing in the test tweets is", len(emoticon_test_tweets))
print("The number of unique EMOTICONS appearing in the test tweets is", len(set(emoticon_test_tweets)))
print("\nUnique EMOTICONS:")
print(set(emoticon_test_tweets),'\n')
print('-'*1000)

print("\nThe total number of USERNAMES appearing in the test tweets is", len(user_test_tweets))
print("The number of unique USERNAMES appearing in the test tweets is", len(set(user_test_tweets)))
print("\nUnique USERNAMES:")
print(set(user_test_tweets), '\n')
print('-'*1000)

print("\nThe total number of URLs appearing in the test tweets is", len(url_test_tweets))
print("The number of unique URLs appearing in the test tweets is", len(set(url_test_tweets)))
print("\nUnique URLs:")
print(set(url_test_tweets))

### Test news ###

user_test_news, hashtag_test_news, url_test_news, emoticon_test_news =  text_analysis(df_test_news)

print("\nThe total number of HASHTAGS appearing in the test news is", len(hashtag_test_news))
if len(hashtag_test_news)!=0:
  print("The number of unique HASHTAGS appearing in the test news is", len(set(hashtag_test_news)))
  print("\nUnique HASHTAGS:")
  print(set(hashtag_test_news))
print()
print('-'*1000)

print("\nThe total number of EMOTICONS appearing in the test news is", len(emoticon_test_news))
if len(emoticon_test_news)!=0:
  print("The number of unique EMOTICONS appearing in the test news is", len(set(emoticon_test_news)))
  print("\nUnique EMOTICONS:")
  print(set(emoticon_test_news))
print()
print('-'*1000)

print("\nThe total number of USERNAMES appearing in the test news is", len(user_test_news))
if len(user_test_news)!=0:
  print("The number of unique USERNAMES appearing in the test news is", len(set(user_test_news)))
  print("\nUnique USERNAMES:")
  print(set(user_test_news))
print()
print('-'*1000)

print("\nThe total number of URLs appearing in the test news is", len(url_test_news))
if len(url_test_news)!=0:
  print("The number of unique URLs appearing in the test news is", len(set(url_test_news)))
  print("\nUnique URLs:")
  print(set(url_test_news))
print()

"""It is important to notice that the number of #hashtags, emoticons, usernames and urls is extremely different with respect to those seen during the tweets inspection.

By inspecting the data, it turned out that some training tweets only contain the "*#ERROR!*" string. Thus, these entries are removed.
"""

# Checking #ERROR! train tweets

df_train[df_train['text'].str.contains('#ERROR')]

print('Original training set size:',df_train.shape)
filtering = df_train['text'].str.contains('#ERROR')
df_train = df_train[~filtering]
print('Training set size after removing #ERROR tweets:', df_train.shape)

"""## Data splitting

The provided training set is split into $80$\% for train and $20$\% for validation.
"""

df_train, df_val = train_test_split(df_train, test_size=0.2, random_state = 42)

df_train = df_train.reset_index(drop=True)
df_val = df_val.reset_index(drop=True)

print('df_train:', df_train.shape)
print('df_val:', df_val.shape)
print('df_test_news:', df_test_news.shape)
print('df_test_tweets:', df_test_tweets.shape)

"""## Classes distribution analysis

To have a better comprehension of the distribution of the classes in the dataset in the hate speech detection task, the support for each class is visualized.
"""

column = 'hate_speech'
print("Distribution of hate_speech classes for training tweets: ")
print(df_train[column].value_counts())
print()

print("Distribution of hate_speech classes for validation tweets: ")
print(df_val[column].value_counts())
print()

print("Distribution of hate_speech classes for test tweets: ")
print(df_test_tweets[column].value_counts())
print()

print("Distribution of hate_speech classes for test news: ")
print(df_test_news[column].value_counts())
print()

"""The same is done for the stereotype detection task."""

column = 'stereotype'
print("Distribution of stereotype classes for training tweets: ")
print(df_train[column].value_counts())
print()

print("Distribution of stereotype classes for validation tweets: ")
print(df_val[column].value_counts())
print()

print("Distribution of stereotype classes for test tweets: ")
print(df_test_tweets[column].value_counts())
print()

print("Distribution of stereotype classes for test news: ")
print(df_test_news[column].value_counts())
print()

column = 'hate_speech'
counts_train = np.asarray(df_train[column].tolist(), dtype=object)
counts_val =  np.asarray(df_val[column].tolist(), dtype=object)
counts_test_tweets =  np.asarray(df_test_tweets[column].tolist(), dtype=object)
counts_test_news =  np.asarray(df_test_news[column].tolist(), dtype=object)

fig, plots = plt.subplots(1,2, sharey=True, figsize=(10, 6))
fig.suptitle('Classes distribution')
plots[0].hist([counts_train, counts_val, counts_test_tweets, counts_test_news],
         bins=np.arange(3),
         label=['train', 'validation', 'test tweets', 'test news'],
         color=["darkred", "tab:orange", "navy", "darkgreen"], align='left')
plots[0].legend(loc='upper center')
plots[0].set_title("Hate speech")
plots[0].set_xticks(range(0,2))

column = 'stereotype'
counts_train = np.asarray(df_train[column].tolist(), dtype=object)
counts_val =  np.asarray(df_val[column].tolist(), dtype=object)
counts_test_tweets =  np.asarray(df_test_tweets[column].tolist(), dtype=object)
counts_test_news =  np.asarray(df_test_news[column].tolist(), dtype=object)

plots[1].hist([counts_train, counts_val, counts_test_tweets, counts_test_news],
         bins=np.arange(3),
         label=['train', 'validation', 'test tweets', 'test news'],
         color=["darkred", "tab:orange", "navy", "darkgreen"], align='left')
plots[1].legend(loc='upper center')
plots[1].set_title("Stereotype")
plots[1].set_xticks(range(0,2))

plt.show()

"""From the above histrograms it can be notice that the two classes, for both tasks, are slightly unbalanced.

# Pre-processing

From the above inspection of the data, it can be seen that the tweets have several symbols and elements that do not contain any information, such as *@user* and *URL*. Therefore, these instances are removed through regular expressions.  
Since *#hashtags* may be more meaningful, they are processed in order to make them more understandable, by removing the
\# character and, as far as possible, splitting those
containing multiple words attached.  
Also punctuation and stopwords are removed, except for "*non*" words, the presence/absence of which can strongly influence the meaning of the sentence.
"""

def cleaning_text(sentences):

  sentences_clean = sentences.replace(r'@[A-Za-z0-9_]+', '', regex=True)
  sentences_clean = sentences_clean.replace(r'\sURL', '', regex=True)
  sentences_clean = sentences_clean.replace(r"[^\w\s]", '', regex=True)

  return sentences_clean

def process_hashtag(input_text):
    return re.sub(
        r'#[A-Za-z0-9_]\S*',
        lambda m: ' '.join(re.findall('[A-Z][^A-Z]*|[a-z][^A-Z]*', m.group().lstrip('#'))),
        input_text,
    )

def apply_process_hashtag(sentences):
  return sentences.apply(lambda x: process_hashtag(x))

def remove_stopwords(sentences):

  return sentences.apply(lambda x: ' '.join([word for word in x.lower().split() if word not in (stopwords_ita)]))

"""Additionally to the basic pre-processing just described, other stages, namely spelling correction and lemmatization are eventually applied."""

def spell_checker(sentences):

  spell_sentences = [spell(sentence) for sentence in sentences]

  return spell_sentences

def lemmatization(sentences):

  lem_sentences = []

  for sentence in sentences:

    flag = True
    while flag:
      try:
        parsing = parse(sentence, lemmata=True).split()[0]
        flag=False
      except(RuntimeError):
        pass

    words = [parsing[i][4] for i in range(0, len(parsing))]
    lem_sentences.append(" ".join(words))

  return lem_sentences

def tokenize_function(sentences):

  tokenizer = TweetTokenizer(strip_handles = True, reduce_len=True, preserve_case=False)

  tokenized_sentences = [tokenizer.tokenize(str(sentence)) for sentence in sentences]

  return tokenized_sentences

def preprocessing_function(df, spell_check, lemma, labels):

  df_copy = df.copy()

  df_copy['clean_text'] = cleaning_text(df_copy['text'])
  df_copy['clean_text'] = apply_process_hashtag(df_copy['clean_text'])
  df_copy['clean_text'] = remove_stopwords(df_copy['clean_text'])

  if spell_check:
    df_copy['clean_text'] = spell_checker(df_copy['clean_text'])

  if lemma:
    df_copy['clean_text'] = lemmatization(df_copy['clean_text'])

  return df_copy['clean_text'], np.array(df_copy[labels].tolist())

nltk.download('punkt')
nltk.download('stopwords')
stopwords_ita = stopwords.words('italian')
spell = Speller('it', fast=True)

# Checking stopwords
stopwords_ita

# Removing "non" from stopwords, since can have useful meaning for the task
print("The original number of stopwords is", len(stopwords_ita))
stopwords_ita.remove('non')
print("The number of stopwords after removing 'non' is", len(stopwords_ita))

"""# ML models

## TF-IDF words representation

A set of dictionaries is initialized to store the results of the ML models in order to compare them.
"""

set_reproducibility(42)

ml_scores_hs_dict = dict()
ml_models_hs_dict = dict()
ml_pred_tweets_hs_dict = dict()
ml_pred_news_hs_dict = dict()

ml_scores_st_dict = dict()
ml_models_st_dict = dict()
ml_pred_tweets_st_dict = dict()
ml_pred_news_st_dict = dict()

"""The `tf_idf_vectorization()` function computes the TF-IDF representation for each word. This is done in order to have suitable input features for the two ML models that will be used."""

def tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news):
  vectorizer =  TfidfVectorizer()

  X_train = vectorizer.fit_transform(sentences_train)
  X_val = vectorizer.transform(sentences_val)
  X_test_tweets = vectorizer.transform(sentences_test_tweets)
  X_test_news = vectorizer.transform(sentences_test_news)


  return X_train, X_val, X_test_tweets, X_test_news

"""## Support Vector Classifier

The first ML model used to classify both hate speech and stereotype language is the Support Vector Classifier.

Through the `svc_classifier()` function transformed data are used to train the SVC model and to make predictions on test tweets and news.
"""

def svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news):

  svc_classifier = LinearSVC(random_state = 42)

  svc_classifier.fit(X_train, labels_train)

  Y_pred_train = svc_classifier.predict(X_train)
  Y_pred_val = svc_classifier.predict(X_val)
  Y_pred_test_tweets = svc_classifier.predict(X_test_tweets)
  Y_pred_test_news = svc_classifier.predict(X_test_news)

  return Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_classifier

"""### Task A

Hate Speech Detection: binary classification task aimed at determining whether the message contains Hate Speech or not.

#### Basic pre-processing

In this section the SVC model is trained and evaluated on data that are only slightly pre-processed, i.e. without applying spelling correction or lemmatization.
"""

spell_check = False
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and the two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['svc'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['svc'] = svc
ml_pred_tweets_hs_dict['svc'] = Y_pred_test_tweets
ml_pred_news_hs_dict['svc'] = Y_pred_test_news


print('\n--------- Classification report Train set -----------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
"""

spell_check = True
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_spell = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['svc_spell'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['svc_spell'] = svc_spell
ml_pred_tweets_hs_dict['svc_spell'] = Y_pred_test_tweets
ml_pred_news_hs_dict['svc_spell'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
"""

spell_check = False
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_lemma = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['svc_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['svc_lemma'] = svc_lemma
ml_pred_tweets_hs_dict['svc_lemma'] = Y_pred_test_tweets
ml_pred_news_hs_dict['svc_lemma'] = Y_pred_test_news

print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also the combination of spelling correction and lemmatization.
"""

spell_check = True
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_spell_lemma = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['svc_spell_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['svc_spell_lemma'] = svc_spell_lemma
ml_pred_tweets_hs_dict['svc_spell_lemma'] = Y_pred_test_tweets
ml_pred_news_hs_dict['svc_spell_lemma'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""### Task B

Stereotype Detection: binary classification task aimed at determining whether the message contains Stereotype or not.

#### Basic pre-processing

In this section the SVC model is trained and evaluated on data that are only slightly pre-processed, i.e. without applying spelling correction or lemmatization.
"""

spell_check = False
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['svc'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['svc'] = svc
ml_pred_tweets_st_dict['svc'] = Y_pred_test_tweets
ml_pred_news_st_dict['svc'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
"""

spell_check = True
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_spell = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['svc_spell'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['svc_spell'] = svc_spell
ml_pred_tweets_st_dict['svc_spell'] = Y_pred_test_tweets
ml_pred_news_st_dict['svc_spell'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
"""

spell_check = False
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_lemma = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['svc_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['svc_lemma'] = svc_lemma
ml_pred_tweets_st_dict['svc_lemma'] = Y_pred_test_tweets
ml_pred_news_st_dict['svc_lemma'] = Y_pred_test_news

print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the SVC model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also the combination of spelling correction and lemmatization.
"""

spell_check = True
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, svc_spell_lemma = svc_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['svc_spell_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['svc_spell_lemma'] = svc_spell_lemma
ml_pred_tweets_st_dict['svc_spell_lemma'] = Y_pred_test_tweets
ml_pred_news_st_dict['svc_spell_lemma'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""## Logistic Regressor

The second ML model used to classify both hate speech and stereotype language is the Logistic Regressor.

Through the `lr_classifier()` function the transformed data are used to train and evaluate the LR model.
"""

def lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news):

  lr_classifier =  LogisticRegression(max_iter=1000, random_state = 42)

  lr_classifier.fit(X_train, labels_train)

  Y_pred_train = lr_classifier.predict(X_train)
  Y_pred_val = lr_classifier.predict(X_val)
  Y_pred_test_tweets = lr_classifier.predict(X_test_tweets)
  Y_pred_test_news = lr_classifier.predict(X_test_news)

  return Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_classifier

"""### Task A

Hate Speech Detection: binary classification task aimed at determining whether the message contains Hate Speech or not.

#### Basic pre-processing

In this section the LR model is trained and evaluated on data that are only slightly pre-processed, i.e. without applying spelling correction or lemmatization.
"""

spell_check = False
lemma= False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['lr'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['lr'] = lr
ml_pred_tweets_hs_dict['lr'] = Y_pred_test_tweets
ml_pred_news_hs_dict['lr'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
"""

spell_check = True
lemma= False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_spell = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['lr_spell'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['lr_spell'] = lr_spell
ml_pred_tweets_hs_dict['lr_spell'] = Y_pred_test_tweets
ml_pred_news_hs_dict['lr_spell'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
"""

spell_check = False
lemma= True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_lemma = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['lr_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['lr_lemma'] = lr_lemma
ml_pred_tweets_hs_dict['lr_lemma'] = Y_pred_test_tweets
ml_pred_news_hs_dict['lr_lemma'] = Y_pred_test_news



print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction and lemmatization.
"""

spell_check = True
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'hate_speech')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_spell_lemma = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_hs_dict['lr_spell_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_hs_dict['lr_spell_lemma'] = lr_spell_lemma
ml_pred_tweets_hs_dict['lr_spell_lemma'] = Y_pred_test_tweets
ml_pred_news_hs_dict['lr_spell_lemma'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""### Task B

Stereotype Detection: binary classification task aimed at determining whether the message contains Stereotype or not.

#### Basic pre-processing

In this section the LR model is trained and evaluated on data that are only slightly pre-processed, i.e. without applying spelling correction or lemmatization.
"""

spell_check = False
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['lr'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['lr'] = lr
ml_pred_tweets_st_dict['lr'] = Y_pred_test_tweets
ml_pred_news_st_dict['lr'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
"""

spell_check = True
lemma = False

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_spell = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['lr_spell'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['lr_spell'] = lr_spell
ml_pred_tweets_st_dict['lr_spell'] = Y_pred_test_tweets
ml_pred_news_st_dict['lr_spell'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
"""

spell_check = False
lemma= True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_lemma = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['lr_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['lr_lemma'] = lr_lemma
ml_pred_tweets_st_dict['lr_lemma'] = Y_pred_test_tweets
ml_pred_news_st_dict['lr_lemma'] = Y_pred_test_news


print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the LR model is trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction and lemmatization.
"""

spell_check = True
lemma = True

sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma,'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma,'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma,'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma,'stereotype')

X_train, X_val, X_test_tweets, X_test_news = tf_idf_vectorization(sentences_train, sentences_val, sentences_test_tweets, sentences_test_news)
Y_pred_train, Y_pred_val, Y_pred_test_tweets, Y_pred_test_news, lr_spell_lemma = lr_classifier(X_train, labels_train, X_val, X_test_tweets, X_test_news)

"""A classification report for the training, validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

report_train = classification_report(labels_train, Y_pred_train)
report_val = classification_report(labels_val, Y_pred_val)
report_test_tweets = classification_report(labels_test_tweets, Y_pred_test_tweets)
report_test_news = classification_report(labels_test_news, Y_pred_test_news)

ml_scores_st_dict['lr_spell_lemma'] = classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)
ml_models_st_dict['lr_spell_lemma'] = lr_spell_lemma
ml_pred_tweets_st_dict['lr_spell_lemma'] = Y_pred_test_tweets
ml_pred_news_st_dict['lr_spell_lemma'] = Y_pred_test_news



print('\n-------- Classification report Train set --------\n')
print(report_train)
print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, Y_pred_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, Y_pred_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, Y_pred_test_news, output_dict = True)['macro avg']['f1-score'])

"""# RNN models

## Embedding

Before feeding DL models, words are represented through a FastText embedding for Italian language, which associates to each word a vector of 300 floating numbers.  
For computational reasons, the FastText model used is a compressed version of the original one.
"""

# Download FastText embedding

ft_ita = compress_fasttext.models.CompressedFastTextKeyedVectors.load("https://zenodo.org/record/4905385/files/fasttext-it-mini?download=1")

embedding_dim = ft_ita[ft_ita.index_to_key[0]].shape[0]
print("The FastText embedding model contains",len(ft_ita.index_to_key),"embeddings of size", embedding_dim)

"""The `find_unique_words()` function returns a list of unique words starting from a set of messages (tweets or news).

"""

def find_unique_words(token_sentences):

  flattened = list(itertools.chain(*token_sentences))
  list_of_words = sorted(list(set(flattened)))

  return list_of_words

"""The `load_vocabulary()` function is used to build a vocabulary that contains all the pairs {word : embedding} using the FastText pre-trained embeddings.


"""

def load_vocabulary(embeddings):

  vocabulary = {}

  for word in embeddings.key_to_index:
    vocabulary[word] = embeddings[word]

  return vocabulary

"""Starting from the FastText vocabulary, a costumed one is created by adding to FastText an embedding for the Out Of Vocabulary (OOV) terms.  
The embeddings for OOV words is obtained by summing up vectors for its component char-ngrams if at least one of the char-ngrams was
present in the train data, creating an embedding vector of 0s otherwise.
"""

def add_OOV_embedding(embeddings, list_of_words, vocabulary):

  """
  For all the words in `list_of_words` not contained into `vocabulary` (OOV terms), compute a random embedding and add it to `vocabulary`.
  """
  vocabulary_copy = vocabulary.copy()

  for word in list_of_words:
    if word not in vocabulary_copy.keys():
      vocabulary_copy[word] = embeddings.word_vec(word)

  return vocabulary_copy

"""The `create_vocabulary` function builds the final vocabulary considering all the words contained in the train, validation and two test sets."""

def create_vocabulary(ft_embeddings, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news):

  print("Loading the vocabulary from the FastText embeddings...")
  vocabulary = load_vocabulary(ft_embeddings)
  print("Done! The size of the vocabulary is", len(vocabulary),'\n')

  print("Adding OOV embeddings to the vocabulary...")
  vocabulary_1 = vocabulary.copy()
  vocabulary_2 = add_OOV_embedding(ft_ita, list_of_words_train, vocabulary_1)
  print("Vocabulary size after train OOV words:", len(vocabulary_2))
  vocabulary_3 = add_OOV_embedding(ft_ita, list_of_words_val, vocabulary_2)
  print("Vocabulary size after val OOV words:  ", len(vocabulary_3))
  vocabulary_4 = add_OOV_embedding(ft_ita, list_of_words_test_tweets, vocabulary_3)
  print("Vocabulary size after test tweets OOV words: ", len(vocabulary_4))
  vocabulary_5 = add_OOV_embedding(ft_ita, list_of_words_test_news, vocabulary_4)
  print("Vocabulary size after test news OOV words: ", len(vocabulary_5))
  vocab_size = len(vocabulary_5)
  print("The final total size of the vocabulary is", vocab_size,'\n')

  return vocabulary_5, vocab_size

"""The `create_embedding_matrix()` function returns a matrix of size ((vocabulary size + 1)  X embedding size).

"""

def create_embedding_matrix(vocabulary, vocab_size, embedding_dim):

  # Creating word:index dictionary
  indexes = np.arange(1, vocab_size+1) # +1 for padding and starting from 1 because 0 index is reserved for padding
  word2id_dict = dict(zip(vocabulary.keys(), indexes))

  # Creating the embedding matrix
  embedding_matrix = np.zeros((vocab_size+1, embedding_dim))

  for word, i in word2id_dict.items():
    embedding_matrix[i] = vocabulary.get(word)

  print("The shape of the created embedding matrix is:", embedding_matrix.shape)

  return word2id_dict, embedding_matrix

"""The `sentences_2_indexes()` function converts each word in the message (tweet or news) in an index. This number will be used as row index in the embedding matrix.


"""

def sentences_2_indexes(sentences, word2id_dict):
    """
    Convert each word in `sentences` into numerical format, according to `word2id_dict`.
    """
    matrix = [np.array([word2id_dict[word] for word in sentence]) for sentence in sentences]
    return matrix

"""## Models definition

In order to obtain fully reproducibility for experiments, it is needed to always reset Tensorflow and Keras backends and to set again the random seed before creating a new Keras model. The `reset_session()` function is in charge of doing that.
"""

def reset_session(seed):

  K.clear_session()
  np.random.seed(seed)
  v1.set_random_seed(seed)
  sess = v1.Session(graph=v1.get_default_graph())
  K.set_session(sess)

"""The first DL model defined is composed by
* the **Input layer**, which takes as input sentences as a sequence of indexes
* an **Embedding layer**, which is a simple lookup table that stores embeddings of each vocabulary word. Given the input as a list of indexes, it retrieves the correspondent word embeddings. In this case, it has been initialized with the `embedding_matrix` previously computed and it is set to be trainable. Through the option `mask_zero = True` it allows the following layers of the network to ignore padding.
* a **Dropout layer** of size $0.5$
* a **Bidirectional LSTM layer** of size $64$
* a **Global Max Pooling 1D layer**
* a **Dense layer** of size $64$ with a ReLU activation function
* a **Dropout layer** of size $0.5$
* a **Dense layer** of size $1$ with a Sigmoid activation function

When compiling the model, the following setup is used:

* Adam optimizer with learning rate of 0.0001
* Binary Cross Entropy Loss
* Accuracy as metric
"""

def create_M1(seed):

  reset_session(seed)

  model = Sequential([
      InputLayer(input_shape=(max_seq_length,)),
      Embedding(vocab_size+1, 300,
                      weights = [embedding_matrix],
                      input_length = (max_seq_length, 300),
                      trainable = True,
                      mask_zero = True),
      Dropout(0.5),
      Bidirectional(LSTM(64, return_sequences=True)),
      GlobalMaxPool1D(),
      Dense(64, activation='relu'),
      Dropout(0.5),
      Dense(1, activation='sigmoid')
      ])

  model.compile(loss='binary_crossentropy',
                optimizer=Adam(0.0001),
                metrics=['accuracy'])

  return model

"""The second DL model defined is composed by
* the **Input layer**, which takes as input sentences as a sequence of indexes
* an **Embedding layer**, which is a simple lookup table that stores embeddings of each vocabulary word. Given the input as a list of indexes, it retrieves the correspondent word embeddings. In this case, it has been initialized with the `embedding_matrix` previously computed and it is set to be trainable. Through the option `mask_zero = True` it allows the following layers of the network to ignore padding.
* a **Dropout layer** of size $0.5$
* a **Bidirectional LSTM layer** of size $16$
* a **Bidirectional LSTM layer** of size $16$
* a **Global Max Pooling 1D layer**
* a **Dense layer** of size $1$ with a Sigmoid activation function

When compiling the model, the following setup is used:

* Adam optimizer with learning rate of 0.0001
* Binary Cross Entropy Loss
* Accuracy as metric
"""

def create_M2(seed):

  reset_session(seed)

  model = Sequential([
      InputLayer(input_shape=(max_seq_length,)),
      Embedding(vocab_size+1, 300,
                      weights = [embedding_matrix],
                      input_length = (max_seq_length, 300),
                      trainable = True,
                      mask_zero = True),
      Dropout(0.5),
      Bidirectional(LSTM(16, return_sequences=True)),
      Bidirectional(LSTM(16, return_sequences=True)),
      GlobalMaxPool1D(),
      Dense(1, activation='sigmoid')
      ])

  model.compile(loss='binary_crossentropy',
                optimizer=Adam(0.0001),
                metrics=['accuracy'])

  return model

"""## Training & evaluation

A set of dictionaries is initialized to store the results of the DL models in order to compare them.
"""

seed = 42
set_reproducibility(seed)

# {model : scores}
rnn_scores_hs_dict = dict()

# {model name : model}
rnn_models_hs_dict = dict()

# {model_name : predictions_tweets}
rnn_pred_tweets_hs_dict = dict()

# {model_name : predictions_news}
rnn_pred_news_hs_dict = dict()

# {model : scores}
rnn_scores_st_dict = dict()

# {model name : model}
rnn_models_st_dict = dict()

# {model_name : predictions_tweets}
rnn_pred_tweets_st_dict = dict()

# {model_name : predictions_news}
rnn_pred_news_st_dict = dict()

"""### Task A

Hate Speech Detection: binary classification task aimed at determining whether the message contains Hate Speech or not.

#### Basic pre-processing

In this section the two RNN models are trained and evaluated on data that are only slightly pre-processed, i.e. without applying spelling correction or lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = False
lemma = False

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'hate_speech')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1 = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1.save('drive/MyDrive/Colab Notebooks/hate_weights/M1')

#M1 = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M1')

predictions_test_tweets = M1.predict(pad_sent_test_tweets)
predictions_val = M1.predict(pad_sent_val)
predictions_test_news = M1.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M1'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M1'] = M1
rnn_pred_tweets_hs_dict['M1'] = classes_test_tweets
rnn_pred_news_hs_dict['M1'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2 = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2.save('drive/MyDrive/Colab Notebooks/hate_weights/M2')

#M2 = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M2')

predictions_test_tweets = M2.predict(pad_sent_test_tweets)
predictions_val = M2.predict(pad_sent_val)
predictions_test_news = M2.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M2'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M2'] = M2
rnn_pred_tweets_hs_dict['M2'] = classes_test_tweets
rnn_pred_news_hs_dict['M2'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = True
lemma = False

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'hate_speech')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_spell = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_spell.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_spell.save('drive/MyDrive/Colab Notebooks/hate_weights/M1_spell')

#M1_spell = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M1_spell')

predictions_test_tweets = M1_spell.predict(pad_sent_test_tweets)
predictions_val = M1_spell.predict(pad_sent_val)
predictions_test_news = M1_spell.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M1_spell'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M1_spell'] = M1_spell
rnn_pred_tweets_hs_dict['M1_spell'] = classes_test_tweets
rnn_pred_news_hs_dict['M1_spell'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_spell = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_spell.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_spell.save('drive/MyDrive/Colab Notebooks/hate_weights/M2_spell')

#M2_spell = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M2_spell')

predictions_test_tweets = M2_spell.predict(pad_sent_test_tweets)
predictions_val = M2_spell.predict(pad_sent_val)
predictions_test_news = M2_spell.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M2_spell'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M2_spell'] = M2_spell
rnn_pred_tweets_hs_dict['M2_spell'] = classes_test_tweets
rnn_pred_news_hs_dict['M2_spell'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = False
lemma = True

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'hate_speech')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_lemma = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_lemma.save('drive/MyDrive/Colab Notebooks/hate_weights/M1_lemma')

#M1_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M1_lemma')

predictions_test_tweets = M1_lemma.predict(pad_sent_test_tweets)
predictions_val = M1_lemma.predict(pad_sent_val)
predictions_test_news = M1_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M1_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M1_lemma'] = M1_lemma
rnn_pred_tweets_hs_dict['M1_lemma'] = classes_test_tweets
rnn_pred_news_hs_dict['M1_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_lemma = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_lemma.save('drive/MyDrive/Colab Notebooks/hate_weights/M2_lemma')

#M2_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M2_lemma')

predictions_test_tweets = M2_lemma.predict(pad_sent_test_tweets)
predictions_val = M2_lemma.predict(pad_sent_val)
predictions_test_news = M2_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M2_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M2_lemma'] = M2_lemma
rnn_pred_tweets_hs_dict['M2_lemma'] = classes_test_tweets
rnn_pred_news_hs_dict['M2_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction and lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = True
lemma = True

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'hate_speech')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'hate_speech')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'hate_speech')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'hate_speech')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_spell_lemma = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_spell_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_spell_lemma.save('drive/MyDrive/Colab Notebooks/hate_weights/M1_spell_lemma')

#M1_spell_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M1_spell_lemma')

predictions_test_tweets = M1_spell_lemma.predict(pad_sent_test_tweets)
predictions_val = M1_spell_lemma.predict(pad_sent_val)
predictions_test_news = M1_spell_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M1_spell_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M1_spell_lemma'] = M1_spell_lemma
rnn_pred_tweets_hs_dict['M1_spell_lemma'] = classes_test_tweets
rnn_pred_news_hs_dict['M1_spell_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_spell_lemma = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_spell_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_spell_lemma.save('drive/MyDrive/Colab Notebooks/hate_weights/M2_spell_lemma')

#M2_spell_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/hate_weights/M2_spell_lemma')

predictions_test_tweets = M2_spell_lemma.predict(pad_sent_test_tweets)
predictions_val = M2_spell_lemma.predict(pad_sent_val)
predictions_test_news = M2_spell_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_hs_dict['M2_spell_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_hs_dict['M2_spell_lemma'] = M2_spell_lemma
rnn_pred_tweets_hs_dict['M2_spell_lemma'] = classes_test_tweets
rnn_pred_news_hs_dict['M2_spell_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""### Task B

Stereotype Detection: binary classification task aimed at determining whether the message contains Stereotype or not.

#### Basic pre-processing

In this section the two DL models are trained and evaluated on data that are only slightly pre-processed, i.e. without spelling correction and lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = False
lemma = False

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'stereotype')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1 = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1.save('drive/MyDrive/Colab Notebooks/stereo_weights/M1')

#M1 = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M1')

predictions_test_tweets = M1.predict(pad_sent_test_tweets)
predictions_val = M1.predict(pad_sent_val)
predictions_test_news = M1.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M1'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M1'] = M1
rnn_pred_tweets_st_dict['M1'] = classes_test_tweets
rnn_pred_news_st_dict['M1'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2 = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2.save('drive/MyDrive/Colab Notebooks/stereo_weights/M2')

#M2 = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M2')

predictions_test_tweets = M2.predict(pad_sent_test_tweets)
predictions_val = M2.predict(pad_sent_val)
predictions_test_news = M2.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M2'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M2'] = M2
rnn_pred_tweets_st_dict['M2'] = classes_test_tweets
rnn_pred_news_st_dict['M2'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = True
lemma = False

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'stereotype')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_spell = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_spell.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_spell.save('drive/MyDrive/Colab Notebooks/stereo_weights/M1_spell')

#M1_spell = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M1_spell')

predictions_test_tweets = M1_spell.predict(pad_sent_test_tweets)
predictions_val = M1_spell.predict(pad_sent_val)
predictions_test_news = M1_spell.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M1_spell'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M1_spell'] = M1_spell
rnn_pred_tweets_st_dict['M1_spell'] = classes_test_tweets
rnn_pred_news_st_dict['M1_spell'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_spell = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_spell.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_spell.save('drive/MyDrive/Colab Notebooks/stereo_weights/M2_spell')

#M2_spell = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M2_spell')

predictions_test_tweets = M2_spell.predict(pad_sent_test_tweets)
predictions_val = M2_spell.predict(pad_sent_val)
predictions_test_news = M2_spell.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M2_spell'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M2_spell'] = M2_spell
rnn_pred_tweets_st_dict['M2_spell'] = classes_test_tweets
rnn_pred_news_st_dict['M2_spell'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Lemmatization

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = False
lemma = True

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'stereotype')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_lemma = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_lemma.save('drive/MyDrive/Colab Notebooks/stereo_weights/M1_lemma')

#M1_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M1_lemma')

predictions_test_tweets = M1_lemma.predict(pad_sent_test_tweets)
predictions_val = M1_lemma.predict(pad_sent_val)
predictions_test_news = M1_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M1_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M1_lemma'] = M1_lemma
rnn_pred_tweets_st_dict['M1_lemma'] = classes_test_tweets
rnn_pred_news_st_dict['M1_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_lemma = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_lemma.save('drive/MyDrive/Colab Notebooks/stereo_weights/M2_lemma')

M2_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M2_lemma')

predictions_test_tweets = M2_lemma.predict(pad_sent_test_tweets)
predictions_val = M2_lemma.predict(pad_sent_val)
predictions_test_news = M2_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M2_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M2_lemma'] = M2_lemma
rnn_pred_tweets_st_dict['M2_lemma'] = classes_test_tweets
rnn_pred_news_st_dict['M2_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Spell-checking & lemmatization

In this section the two DL models are trained and evaluated on data which underwent, in addition to the basic pre-processing, also spelling correction and lemmatization.
In addition, after the pre-processing phase, data are tokenized using the `tokenize_function`, which uses the `TweetTokenizer` from nltk package. After tokenization, a the list of unique words is built.
"""

spell_check = True
lemma = True

# Preprocessing sentences
sentences_train, labels_train = preprocessing_function(df_train, spell_check, lemma, 'stereotype')
sentences_val, labels_val = preprocessing_function(df_val, spell_check, lemma, 'stereotype')
sentences_test_tweets, labels_test_tweets = preprocessing_function(df_test_tweets, spell_check, lemma, 'stereotype')
sentences_test_news, labels_test_news = preprocessing_function(df_test_news, spell_check, lemma, 'stereotype')

# Tokenization
token_sentences_train = tokenize_function(sentences_train)
token_sentences_val = tokenize_function(sentences_val)
token_sentences_test_tweets = tokenize_function(sentences_test_tweets)
token_sentences_test_news = tokenize_function(sentences_test_news)

# Unique words
list_of_words_train = find_unique_words(token_sentences_train)
list_of_words_val = find_unique_words(token_sentences_val)
list_of_words_test_tweets = find_unique_words(token_sentences_test_tweets)
list_of_words_test_news = find_unique_words(token_sentences_test_news)

"""Using lists of unique words, the vocabulary is created and then used to build the embedding matrix that will be used as initialization for the embedding layer in the models."""

# Create the vocabulary
vocabulary, vocab_size = create_vocabulary(ft_ita, embedding_dim, list_of_words_train, list_of_words_val, list_of_words_test_tweets, list_of_words_test_news)

# Create embedding matrix
word2id_dict, embedding_matrix = create_embedding_matrix(vocabulary, vocab_size, embedding_dim)

"""Then each sample is converted in numerical format."""

# Sentences conversion into numerical format
ids_word_train = sentences_2_indexes(token_sentences_train, word2id_dict)
ids_word_val = sentences_2_indexes(token_sentences_val, word2id_dict)
ids_word_test_tweets = sentences_2_indexes(token_sentences_test_tweets, word2id_dict)
ids_word_test_news = sentences_2_indexes(token_sentences_test_news, word2id_dict)

print("Original sentence:", sentences_train[0])
print("Numerical format:", list(ids_word_train[0]))

"""The messages in the corpus are not of the same length. Before feeding the input in the models, it's needed to fix the length of the tweets and the news. Therefore, the next step after encoding the data is to define the maximum sequence length. It has been decided to compute `max_seq_length` as to handle the 99% of the training tweets."""

max_seq_length = int(np.quantile([len(seq) for seq in token_sentences_train], 0.99))
print('The computed sentence maximum length is', max_seq_length)

"""Then, all the sentences shorter than `max_seq_length` have been padded with as many 0s as to reach the maximum length, while the longer ones have simply been truncated."""

pad_sent_train = pad_sequences(ids_word_train, maxlen=max_seq_length, padding='post')
pad_sent_val = pad_sequences(ids_word_val, maxlen=max_seq_length, padding='post')
pad_sent_test_tweets = pad_sequences(ids_word_test_tweets, maxlen=max_seq_length, padding='post')
pad_sent_test_news = pad_sequences(ids_word_test_news, maxlen=max_seq_length, padding='post')

print("Padded sentence:")
print(list(pad_sent_train[0]))

"""##### Modello 1

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M1_spell_lemma = create_M1(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M1_spell_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M1_spell_lemma.save('drive/MyDrive/Colab Notebooks/stereo_weights/M1_spell_lemma')

#M1_spell_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M1_spell_lemma')

predictions_test_tweets = M1_spell_lemma.predict(pad_sent_test_tweets)
predictions_val = M1_spell_lemma.predict(pad_sent_val)
predictions_test_news = M1_spell_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M1_spell_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M1_spell_lemma'] = M1_spell_lemma
rnn_pred_tweets_st_dict['M1_spell_lemma'] = classes_test_tweets
rnn_pred_news_st_dict['M1_spell_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""##### Modello 2

The training is performed for a maximum of 100 epochs, using the early stopping technique with `patience = 3`, and with a batch size of 32.
"""

M2_spell_lemma = create_M2(seed)
callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = M2_spell_lemma.fit(pad_sent_train, labels_train, batch_size=32, epochs=100, validation_data=(pad_sent_val, labels_val), callbacks = [callback])
M2_spell_lemma.save('drive/MyDrive/Colab Notebooks/stereo_weights/M2_spell_lemma')

#M2_spell_lemma = keras.models.load_model('drive/MyDrive/Colab Notebooks/stereo_weights/M2_spell_lemma')

predictions_test_tweets = M2_spell_lemma.predict(pad_sent_test_tweets)
predictions_val = M2_spell_lemma.predict(pad_sent_val)
predictions_test_news = M2_spell_lemma.predict(pad_sent_test_news)

"""A classification report for validation and two test sets is computed in order to have a first impression on the obtained results using this type of pre-processing."""

classes_val = ((predictions_val > 0.5)+0).ravel()
classes_test_tweets = ((predictions_test_tweets > 0.5)+0).ravel()
classes_test_news = ((predictions_test_news > 0.5)+0).ravel()

report_val = classification_report(labels_val, classes_val)
report_test_tweets = classification_report(labels_test_tweets, classes_test_tweets)
report_test_news = classification_report(labels_test_news, classes_test_news)

rnn_scores_st_dict['M2_spell_lemma'] = classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)
rnn_models_st_dict['M2_spell_lemma'] = M2_spell_lemma
rnn_pred_tweets_st_dict['M2_spell_lemma'] = classes_test_tweets
rnn_pred_news_st_dict['M2_spell_lemma'] = classes_test_news

print('\n-------- Classification report Validation set --------\n')
print(report_val)
print('\n-------- Classification report Test set tweets --------\n')
print(report_test_tweets)
print('\n-------- Classification report Test set news --------\n')
print(report_test_news)

print("Macro F1-score on the validation set:", classification_report(labels_val, classes_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(labels_test_tweets, classes_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(labels_test_news, classes_test_news, output_dict = True)['macro avg']['f1-score'])

"""# Transformers

A set of dictionaries is initialized to store the results of the models in order to compare them.
"""

trans_scores_hs_dict = dict()
trans_models_hs_dict = dict()
trans_pred_tweets_hs_dict = dict()
trans_pred_news_hs_dict = dict()

trans_scores_st_dict = dict()
trans_models_st_dict = dict()
trans_pred_tweets_st_dict = dict()
trans_pred_news_st_dict = dict()

dataset_train = Dataset.from_pandas(df_train)
dataset_val = Dataset.from_pandas(df_val)
dataset_test_tweets = Dataset.from_pandas(df_test_tweets)
dataset_test_news = Dataset.from_pandas(df_test_news)

id2label = {0: "NEGATIVE", 1: "POSITIVE"}
label2id = {"NEGATIVE": 0, "POSITIVE": 1}

"""It is important to underline that the input does not underwent any kind of pre-processing or cleaning before feeding the transformers.

## Utility functions

The `preprocess_function` is in charge of tokenizing and preparing the models inputs.  
In particular, inputs cannot exceed `max_length`, therfore all the messages are truncated or padded accordingly.
"""

def preprocess_function(batch, max_length, labels):

  inputs = tokenizer(batch["text"], truncation=True, max_length = max_length)
  batch["input_ids"] = inputs.input_ids
  batch["attention_mask"] = inputs.attention_mask
  batch["labels"] = batch[labels].copy()

  return batch

"""The `tokenize_dataset` makes use of the `map` function to transform each training and validation batch into a batch of model inputs."""

def tokenize_dataset(model_name, dataset_train, dataset_val, max_length, labels):

  tokenized_train = dataset_train.map(preprocess_function,
                                    batched=True,
                                    remove_columns = ['id', 'text', 'stereotype', 'hate_speech'],
                                    fn_kwargs = {'max_length' : max_length, 'labels' : labels})

  tokenized_val = dataset_val.map(preprocess_function,
                                batched=True,
                                remove_columns = ['id', 'text', 'stereotype', 'hate_speech'],
                                fn_kwargs = {'max_length' : max_length, 'labels' : labels})

  return tokenized_train, tokenized_val

"""The `compute_metric` function evaluates the model on the validation set using the accuracy. In particular, it takes as input the true labels and the obtained predictions. Then, it computes the accuracy."""

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
  predictions, labels = eval_pred
  predictions = np.argmax(predictions, axis=1)
  return accuracy.compute(predictions=predictions, references=labels)

"""The fine-tuning of models is carried out instantiating a `Trainer` object. Then, the output is passed as predictions to the `compute_metric` function, which can compute on the validation set the desired metric.

Before starting the training, the `train_model` function configures the `TrainingArguments`, by setting a $3$ epochs fine-tuning, using a learning rate equal to $2e^{-5}$, a batch size of 64, using a weight decay for reguralrization equal to 0.01 and AdamW as optimizer. Out of several experimental results, this setting was found to be the most effective. Then, `Trainer` is runned on the tokenized training and validation sets and the model fine-tuning starts.

Fine-tuned model and weights are finally saved for later reload.

"""

def train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, save_path, output_dir):

  training_args = TrainingArguments(
      output_dir = output_dir,
      learning_rate=2e-5,
      per_device_train_batch_size=64,
      per_device_eval_batch_size=64,
      num_train_epochs=3,
      weight_decay=0.01,
      evaluation_strategy="epoch",
      save_strategy="epoch",
      logging_steps=2,
      load_best_model_at_end=True)

  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_train,
      eval_dataset=tokenized_val,
      tokenizer=tokenizer,
      data_collator=data_collator,
      compute_metrics=compute_metrics)

  trainer.train()

  trainer.save_model(save_path)

"""`model_predictions` computes the predictions for the data passed as input, deploying the fine tuned model and the tokenizer.


"""

def model_predictions(model, tokenizer, dataset):

  classifier = pipeline("text-classification", model = model, tokenizer = tokenizer, device = 0)
  results = classifier(dataset['text'])
  predictions = [label2id[result['label']] for result in results]

  return predictions

def check_passage_answer_max_length(tokenizer, df_train, perc):

  """
  Computes the maximum text length of a chosen percentage of training data.
  """

  text_max_length = np.percentile(df_train.loc[:, ['text']].applymap(lambda text: len(tokenizer(text).input_ids)), perc)

  print('Text max length: ' + str(text_max_length))

  return text_max_length

def reload_model(path, model_name):

  """
  Loads from `path` the model whose `model_name` is passed as input.
  """

  model = AutoModelForSequenceClassification.from_pretrained(path)
  model.to('cuda')
  tokenizer = AutoTokenizer.from_pretrained(model_name)

  return model, tokenizer

"""## UmBERTo

UmBERTo is a Roberta-based Language Model trained on the cased version
of the Italian subcorpus of OSCAR. It is fine-tuned and evaluated using $2$ different seeds ($42$, $2023$).  

The UmBERTo model and tokenizer are loaded from HuggingFace.
The generic tokenizer class `AutoTokenizer` allows to instantiate a predefined tokenizer hosted inside the pretrained model repo on HuggingFace. This can be done using the `from_pretrained` method and passing the pre-trained model name.
"""

model_name = 'Musixmatch/umberto-commoncrawl-cased-v1'
tokenizer = AutoTokenizer.from_pretrained(model_name)

text_max_length = check_passage_answer_max_length(tokenizer, df_train, 99)

"""### Task A

Hate Speech Detection: binary classification task aimed at determining whether the message contains Hate Speech or not.

#### Seed 42
"""

set_reproducibility(42)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'hate_speech')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/hate_weights/umberto_42', 'hate_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/hate_weights/umberto_42', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['hate_speech'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['hate_speech'], predictions_test_news))

trans_scores_hs_dict['umberto_42']  = classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict=True)
trans_models_hs_dict['umberto_42'] = model
trans_pred_tweets_hs_dict['umberto_42'] = predictions_test_tweets
trans_pred_news_hs_dict['umberto_42'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['hate_speech'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['hate_speech'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Seed 2023"""

set_reproducibility(2023)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'hate_speech')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/hate_weights/umberto_2023', 'hate_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/hate_weights/umberto_2023', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['hate_speech'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['hate_speech'], predictions_test_news))

trans_scores_hs_dict['umberto_2023']  = classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict=True)
trans_models_hs_dict['umberto_2023'] = model
trans_pred_tweets_hs_dict['umberto_2023'] = predictions_test_tweets
trans_pred_news_hs_dict['umberto_2023'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['hate_speech'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['hate_speech'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""### Task B

Stereotype Detection: binary classification task aimed at determining whether the message contains Stereotype or not.

#### Seed 42

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $42$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(42)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'stereotype')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/stereo_weights/umberto_42', 'stereo_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/stereo_weights/umberto_42', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['stereotype'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['stereotype'], predictions_test_news))

trans_scores_st_dict['umberto_42']  = classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict=True)
trans_models_st_dict['umberto_42'] = model
trans_pred_tweets_st_dict['umberto_42'] = predictions_test_tweets
trans_pred_news_st_dict['umberto_42'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['stereotype'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['stereotype'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Seed 2023

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $42$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(2023)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'stereotype')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/stereo_weights/umberto_2023', 'stereo_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/stereo_weights/umberto_2023', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['stereotype'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['stereotype'], predictions_test_news))

trans_scores_st_dict['umberto_2023']  = classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict=True)
trans_models_st_dict['umberto_2023'] = model
trans_pred_tweets_st_dict['umberto_2023'] = predictions_test_tweets
trans_pred_news_st_dict['umberto_2023'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['stereotype'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['stereotype'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""## DistilBERT-multilingual

DistilBERT-multilingual is a distilled version of the BERT base multilingual. It is trained on the concatenation of Wikipedia in
104 different language.  
It is fine-tuned and evaluated using $2$ different seeds ($42$, $2023$).


The DistilBERT model and tokenizer are loaded from HuggingFace.
The generic tokenizer class `AutoTokenizer` allows to instantiate a predefined tokenizer hosted inside the pretrained model repo on HuggingFace. This can be done using the `from_pretrained` method and passing the pre-trained model name.
"""

model_name = "distilbert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

text_max_length = check_passage_answer_max_length(tokenizer, df_train, 99)

"""### Task A

Hate Speech Detection: binary classification task aimed at determining whether the message contains Hate Speech or not.

#### Seed 42

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $42$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(42)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

"""Then the train and validation sets are tokenized and a Data collator is defined.  
Data collectors are objects that form a batch by using a list of dataset elements as input. To be able to build batches, data collators apply some processing, in this case padding.
"""

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'hate_speech')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/hate_weights/distilbert_42', 'hate_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/hate_weights/distilbert_42', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['hate_speech'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['hate_speech'], predictions_test_news))

trans_scores_hs_dict['distilbert_42']  = classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict=True)
trans_models_hs_dict['distilbert_42'] = model
trans_pred_tweets_hs_dict['distilbert_42'] = predictions_test_tweets
trans_pred_news_hs_dict['distilbert_42'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['hate_speech'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['hate_speech'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Seed 2023

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $2023$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(2023)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

"""Then the train and validation sets are tokenized and a Data collator is defined.  
Data collectors are objects that form a batch by using a list of dataset elements as input. To be able to build batches, data collators apply some processing, in this case padding.
"""

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'hate_speech')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/hate_weights/distilbert_2023', 'hate_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/hate_weights/distilbert_2023', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['hate_speech'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['hate_speech'], predictions_test_news))

trans_scores_hs_dict['distilbert_2023']  = classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict=True)
trans_models_hs_dict['distilbert_2023'] = model
trans_pred_tweets_hs_dict['distilbert_2023'] = predictions_test_tweets
trans_pred_news_hs_dict['distilbert_2023'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['hate_speech'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['hate_speech'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['hate_speech'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""### Task B

Stereotype Detection: binary classification task aimed at determining whether the message contains Stereotype or not.

#### Seed 42

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $42$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(42)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

"""Then the train and validation sets are tokenized and a Data collator is defined.  
Data collectors are objects that form a batch by using a list of dataset elements as input. To be able to build batches, data collators apply some processing, in this case padding.
"""

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'stereotype')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/stereo_weights/distilbert_42', 'stereo_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/stereo_weights/distilbert_42', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['stereotype'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['stereotype'], predictions_test_news))

trans_scores_st_dict['distilbert_42']  = classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict=True)
trans_models_st_dict['distilbert_42'] = model
trans_pred_tweets_st_dict['distilbert_42'] = predictions_test_tweets
trans_pred_news_st_dict['distilbert_42'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['stereotype'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['stereotype'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""#### Seed 2023

Before training the model, it's important to set the reproducibility, in this case using a seed equal to $2023$. Moreover the model is instantiated  using the `from_pretrained` method of the `AutoModelForSequenceClassification` class.
"""

set_reproducibility(2023)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label=id2label, label2id=label2id)

"""Then the train and validation sets are tokenized and a Data collator is defined.  
Data collectors are objects that form a batch by using a list of dataset elements as input. To be able to build batches, data collators apply some processing, in this case padding.
"""

tokenized_train, tokenized_val = tokenize_dataset(model_name, dataset_train, dataset_val, int(text_max_length), 'stereotype')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

train_model(model, tokenized_train, tokenized_val, tokenizer, data_collator, 'drive/MyDrive/Colab Notebooks/stereo_weights/distilbert_2023', 'stereo_weights')

#model, tokenizer = reload_model('drive/MyDrive/Colab Notebooks/stereo_weights/distilbert_2023', model_name)

"""A classification report for the validation and two test sets is computed in order to have a first impression on the obtained results with this type of pre-processing."""

predictions_val = model_predictions(model, tokenizer, dataset_val)
print('-------- Classification report Validation set --------\n')
print(classification_report(dataset_val['stereotype'], predictions_val))

predictions_test_tweets = model_predictions(model, tokenizer, dataset_test_tweets)
print('\n-------- Classification report Test set tweets --------\n')
print(classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets))

predictions_test_news = model_predictions(model, tokenizer, dataset_test_news)
print('\n-------- Classification report Test set news --------\n')
print(classification_report(dataset_test_news['stereotype'], predictions_test_news))

trans_scores_st_dict['distilbert_2023']  = classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict=True)
trans_models_st_dict['distilbert_2023'] = model
trans_pred_tweets_st_dict['distilbert_2023'] = predictions_test_tweets
trans_pred_news_st_dict['distilbert_2023'] = predictions_test_news

print("Macro F1-score on the validation set:", classification_report(dataset_val['stereotype'], predictions_val, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the tweets test set:", classification_report(dataset_test_tweets['stereotype'], predictions_test_tweets, output_dict = True)['macro avg']['f1-score'])
print("Macro F1-score on the news test set:", classification_report(dataset_test_news['stereotype'], predictions_test_news, output_dict = True)['macro avg']['f1-score'])

"""# Error Analysis

For both tasks, to better reason on errors causes, the combinations of model and preprocessing that yield the best results on test tweets are chosen, one per model type. Then, the analysis focuses on samples which are misclassified by all selected models, in order to investigate what the source of error involved might be.

## Task A

### Best models selection
"""

def models_f1_score(model_type, scores_dict, models_dict):

    """
    Prints macro f1-scores for the given models.

    Parameters:
    - model_type (str): The type of the models.
    - scores_dict (dict): A dictionary containing the evaluation results for each model.
    - models_dict (dict): A dictionary containing the models.

    Returns:
    - f1_scores_macro_dict (dict): A dictionary containing the macro f1-scores of each model.
    """

    f1_scores_macro_dict = {}
    rows = [[f'{model_type} model','macro f1-score'],['','']]

    for model in models_dict.keys():
      f1_scores_macro_dict[model] = scores_dict[model]['macro avg']['f1-score']
      rows.append([model, scores_dict[model]['macro avg']['f1-score']])

    lens = []
    for col in zip(*rows):
      lens.append(max([len(str(v)) for v in col]))
    format = "  ".join(["{:<" + str(l) + "}" for l in lens])

    for row in rows:
      print(format.format(*row))

    return f1_scores_macro_dict

"""First, let's print the macro f1-score of all models on the test tweets."""

trans_f1_macro_dict = models_f1_score('Transformer', trans_scores_hs_dict, trans_models_hs_dict)
print()
ml_f1_macro_dict = models_f1_score('Classic ML', ml_scores_hs_dict, ml_models_hs_dict)
print()
rnn_f1_macro_dict = models_f1_score('RNN', rnn_scores_hs_dict, rnn_models_hs_dict)

"""Then, let's order them by macro f1-score for selecting only the best one per model type."""

sorted_f1_scores_rnn = sorted(rnn_f1_macro_dict, key=rnn_f1_macro_dict.get, reverse=True)
sorted_f1_scores_trans = sorted(trans_f1_macro_dict, key=trans_f1_macro_dict.get, reverse=True)
sorted_f1_scores_ml = sorted(ml_f1_macro_dict, key=ml_f1_macro_dict.get, reverse=True)

best_rnn_model = sorted_f1_scores_rnn[0]
best_trans_model = sorted_f1_scores_trans[0]
best_ml_model = sorted_f1_scores_ml[0]

print('According to the macro f1-score on the test set of tweets, the best Transformer model is:', best_trans_model)
print('According to the macro f1-score on the test set of tweets, the best classic ML model is:', best_ml_model)
print('According to the macro f1-score on the test set of tweets, the best RNN model is:', best_rnn_model)

"""### F1-score, precision and recall

In order to compare the best models results, it is useful to plot the macro f1-score, precision and recall obtained on both on the test tweets and the test news.
"""

pred_tweets_trans = trans_pred_tweets_hs_dict[best_trans_model]
pred_tweets_ml =  ml_pred_tweets_hs_dict[best_ml_model]
pred_tweets_rnn = rnn_pred_tweets_hs_dict[best_rnn_model]

prfs_trans = prfs(df_test_tweets['hate_speech'], pred_tweets_trans, average="macro")
prfs_ml = prfs(df_test_tweets['hate_speech'], pred_tweets_ml, average="macro")
prfs_rnn = prfs(df_test_tweets['hate_speech'], pred_tweets_rnn, average="macro")

fig, plots = plt.subplots(1,3, sharey=True, figsize=(18, 6))
fig.suptitle('Macro scores comparison - Test tweets')
plots[0].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[0], prfs_rnn[0], prfs_trans[0]], color=["darkred", "tab:orange", "navy"])
plots[0].set_title("Precision macro")
plots[1].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[1], prfs_rnn[1], prfs_trans[1]], color=["darkred", "tab:orange", "navy"])
plots[1].set_title("Recall macro")
plots[2].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[2], prfs_rnn[2], prfs_trans[2]], color=["darkred", "tab:orange", "navy"])
plots[2].set_title("Macro f1-score")

plt.show()

print()

pred_news_trans = trans_pred_news_hs_dict[best_trans_model]
pred_news_ml =  ml_pred_news_hs_dict[best_ml_model]
pred_news_rnn = rnn_pred_news_hs_dict[best_rnn_model]

prfs_trans = prfs(df_test_news['hate_speech'], pred_news_trans, average="macro")
prfs_ml = prfs(df_test_news['hate_speech'], pred_news_ml, average="macro")
prfs_rnn = prfs(df_test_news['hate_speech'], pred_news_rnn, average="macro")

fig, plots = plt.subplots(1,3, sharey=True, figsize=(18, 6))
fig.suptitle('Macro scores comparison - Test news')
plots[0].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[0], prfs_rnn[0], prfs_trans[0]], color=["darkred", "tab:orange", "navy"])
plots[0].set_title("Precision macro")
plots[1].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[1], prfs_rnn[1], prfs_trans[1]], color=["darkred", "tab:orange", "navy"])
plots[1].set_title("Recall macro")
plots[2].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[2], prfs_rnn[2], prfs_trans[2]], color=["darkred", "tab:orange", "navy"])
plots[2].set_title("Macro f1-score")

plt.show()

"""Given the same test set type, results of the three best models are fully comparable. However, while there is no significant difference between precision, recall and f1-score of models on test tweets, on test news they show an higher precision, at the expense of low recall and macro f1-score."""

prfs_1 = prfs(df_test_tweets['hate_speech'], pred_tweets_trans, labels=[0,1], average=None, zero_division=0)
prfs_2 = prfs(df_test_tweets['hate_speech'], pred_tweets_ml, labels=[0,1], average=None, zero_division=0)
prfs_3 = prfs(df_test_tweets['hate_speech'], pred_tweets_rnn, labels=[0,1], average=None, zero_division=0)

prfs_4 = prfs(df_test_news['hate_speech'], pred_news_trans, labels=[0,1], average=None, zero_division=0)
prfs_5 = prfs(df_test_news['hate_speech'], pred_news_ml, labels=[0,1], average=None, zero_division=0)
prfs_6 = prfs(df_test_news['hate_speech'], pred_news_rnn, labels=[0,1], average=None, zero_division=0)

width=0.15

ids = np.arange(0,2)
x_labels = [0, 1]

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('Precision')
ax[0].bar(ids-5/4*width, prfs_1[0], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[0], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[0], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[0], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[0], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[0], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')


plt.show()
print()

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('Recall')
ax[0].bar(ids-5/4*width, prfs_1[1], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[1], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[1], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[1], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[1], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[1], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')

plt.show()
print()

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('F1-score')
ax[0].bar(ids-5/4*width, prfs_1[2], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[2], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[2], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[2], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[2], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[2], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')

plt.show()

"""For each model, the per-class precision, recall and f1-score on test tweets are quite consistent with the macro ones, while it is not the case for test news. Indeed, the per-class recall and f1-score of all models on test news are higher on the class $0$ than on the class $1$, which they struggle a lot to classify correctly. On the other hand, precision is higher on the class $1$ than on class $0$.  
This could be influenced by the slight skewing of the training data towards the class $0$, but it's more likely due to the difficulty of identifying hate speech within a different type of language than that used for training.

### Confusion matrices

As a further analysis, let's visualize the confusion matrices for all the models, both on tweets and news test sets.
"""

fig, axn = plt.subplots(1,3, sharey=True, figsize=(20, 5))

fig.suptitle('Test tweets confusion matrices')

c1 = confusion_matrix(df_test_tweets['hate_speech'], pred_tweets_ml, normalize='true', labels=[0,1])
h1 = sns.heatmap(c1, annot = True, fmt = '.2f', ax = axn.flat[0], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h1.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[0].set_title(best_ml_model, fontsize=12)

c2 = confusion_matrix(df_test_tweets['hate_speech'], pred_tweets_rnn, normalize='true', labels=[0,1])
h2 = sns.heatmap(c2, annot = True, fmt = '.2f', ax = axn.flat[1], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h2.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[1].set_title(best_rnn_model, fontsize=12)

c3 = confusion_matrix(df_test_tweets['hate_speech'], pred_tweets_trans, normalize='true', labels=[0,1])
h3 = sns.heatmap(c3, annot = True, fmt = '.2f', ax = axn.flat[2], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h3.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[2].set_title(best_trans_model, fontsize=12)

plt.show()


print()


fig, axn = plt.subplots(1,3, sharey=True, figsize=(20, 5))

fig.suptitle('Test news confusion matrices')

c1 = confusion_matrix(df_test_news['hate_speech'], pred_news_ml, normalize='true', labels=[0,1])
h1 = sns.heatmap(c1, annot = True, fmt = '.2f', ax = axn.flat[0], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h1.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[0].set_title(best_ml_model, fontsize=12)


c2 = confusion_matrix(df_test_news['hate_speech'], pred_news_rnn, normalize='true', labels=[0,1])
h2 = sns.heatmap(c2, annot = True, fmt = '.2f', ax = axn.flat[1], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h2.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[1].set_title(best_rnn_model, fontsize=12)

c3 = confusion_matrix(df_test_news['hate_speech'], pred_news_trans, normalize='true', labels=[0,1])
h3 = sns.heatmap(c3, annot = True, fmt = '.2f', ax = axn.flat[2], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h3.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[2].set_title(best_trans_model, fontsize=12)

plt.show()

"""From those plots it is even clearer that models really struggle in identifying correctly news containing hate speech.

### Specific errors analysis

Let's now have a deeper look into samples which are completely misclassified by all models.

First, it can be useful to print some statistics about the most frequent words in all the sets.
"""

def most_frequent_words(df, n_print, split):

  """
    After applying basic pre-processing and removing stopwords, it finds the most frequent words in the given DataFrame and prints the top `n_print` words.

    Parameters:
    - df (pandas.DataFrame): The DataFrame containing the text data.
    - n_print (int): The number of most frequent words to be printed.
    - split (str): The name of the split to be included in the output.

    Returns:
    - list: A list of tuples, where each tuple contains a word and its frequency, sorted in descending order of frequency.

  """

  wordcount = {}

  prova = df['text'].copy()
  prova = cleaning_text(prova)
  prova = remove_stopwords(prova)

  for p in prova:
    for word in p.lower().split():
        if word != 'non':
          if word not in wordcount:
              wordcount[word] = 1
          else:
              wordcount[word] += 1

  print("\n{} SET".format(split))
  word_counter = collections.Counter(wordcount)
  for word, count in word_counter.most_common(n_print):
      print(word, ": ", count)
  print()

  return word_counter.most_common(n_print)

df_train_hs_1 = df_train[df_train['hate_speech']==1].copy()
df_val_hs_1 = df_val[df_val['hate_speech']==1].copy()
df_test_tweets_hs_1 = df_test_tweets[df_test_tweets['hate_speech']==1].copy()
df_test_news_hs_1 = df_test_news[df_test_news['hate_speech']==1].copy()

n_print = 10
print(f"Most {n_print} common words among all the tweets/news marked as hate speech, for each set:")

train_wc_hs_1 = most_frequent_words(df_train_hs_1, n_print, 'TRAINING')
val_wc_hs_1 = most_frequent_words(df_val_hs_1, n_print, 'VALIDATION')
test_tweets_wc_hs_1 = most_frequent_words(df_test_tweets_hs_1, n_print, 'TWEETS TEST')
test_news_wc_hs_1 = most_frequent_words(df_test_news_hs_1, n_print, 'NEWS TEST')

df_train_hs_0 = df_train[df_train['hate_speech']==0].copy()
df_val_hs_0 = df_val[df_val['hate_speech']==0].copy()
df_test_tweets_hs_0 = df_test_tweets[df_test_tweets['hate_speech']==0].copy()
df_test_news_hs_0 = df_test_news[df_test_news['hate_speech']==0].copy()

n_print = 10
print(f"Most {n_print} common words among all the tweets/news not marked as hate speech, for each set:")

train_wc_hs_0 = most_frequent_words(df_train_hs_0, n_print, 'TRAINING')
val_wc_hs_0 = most_frequent_words(df_val_hs_0, n_print, 'VALIDATION')
test_tweets_wc_hs_0 = most_frequent_words(df_test_tweets_hs_0, n_print, 'TWEETS TEST')
test_news_wc_hs_0 = most_frequent_words(df_test_news_hs_0, n_print, 'NEWS TEST')

train_hs_1 = [train_wc_hs_1[i][0] for i in range(0, len(train_wc_hs_1))]
train_hs_0 = [train_wc_hs_0[i][0] for i in range(0, len(train_wc_hs_0))]

"""The most informative words are those that appear frequently in train tweets belonging to one class, but not in those of the other."""

print("Words that are frequent in hate speech train tweets but not in non-hate speech ones:")
words_hs = [w for w in train_hs_1 if w not in train_hs_0]
words_hs

print("Words that are frequent in non-hate speech train tweets but not in hate speech ones:")
words_not_hs = [w for w in train_hs_0 if w not in train_hs_1]
words_not_hs

"""#### Tweets

For the purpose of analysis, it is needed to retrieve best models predictions.

Also, it might be useful to visualize which kind of input the models have been fed with. So, let's also retrieve the pre-processed inputs, according to the models under analysis.
"""

if 'spell_lemma' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=True, labels = 'hate_speech')
elif 'spell' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=False, labels = 'hate_speech')
elif 'lemma' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=True, labels = 'hate_speech')
else:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=False, labels = 'hate_speech')


if 'spell_lemma' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=True, labels = 'hate_speech')
elif 'spell' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=False, labels = 'hate_speech')
elif 'lemma' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=True, labels = 'hate_speech')
else:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=False, labels = 'hate_speech')

"""For conveniency reasons, all predictions and preprocessed tweets are collected in a single dataframe, which contains also original tweets and true HS labels."""

df_analysis = df_test_tweets.copy()

df_analysis['pred_rnn'] = pred_tweets_rnn
df_analysis['pred_ml'] = pred_tweets_ml
df_analysis['pred_trans'] = pred_tweets_trans
df_analysis['clean_tweets_ml'] = clean_tweets_ml
df_analysis['clean_tweets_rnn'] = clean_tweets_rnn

df_analysis = df_analysis.drop(['clean_text', 'stereotype'], axis=1)
df_analysis

"""Now let's select only those samples that are misclassified by all models under analysis."""

df_all_wrong = df_analysis[(df_analysis['pred_rnn']==df_analysis['pred_ml']) & (df_analysis['pred_rnn']==df_analysis['pred_trans']) & (df_analysis['hate_speech']!=df_analysis['pred_ml'])]
df_all_wrong = df_all_wrong.reset_index(drop=True)
df_all_wrong

"""Then only print some of those that do not contain hate speech (whose HS label is $0$)."""

sample = df_all_wrong[df_all_wrong['hate_speech']==0]
sample = sample.reset_index(drop=True)

n = 23

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['hate_speech'], '\n')
  print('ML input:', sample.loc[i]['clean_tweets_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""Now instead only print some of those that actually contain hate speech (whose HS label is $1$)."""

sample = df_all_wrong[df_all_wrong['hate_speech']==1]
sample = sample.reset_index(drop=True)
n = 23

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['hate_speech'], '\n')
  print('ML input:', sample.loc[i]['clean_tweets_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""Looking deeper into misclassified samples from all models, some interesting possible causes of error have emerged.

For example, it's really hard for models to detect hate speech in tweets where the hate message is somehow only implied and the meaning is hidden behind nuance of the language. Therefore, tweets such as:

"""

sample = df_all_wrong[df_all_wrong['hate_speech']==1]
sample = sample.reset_index(drop=True)
i = 23

print(sample.loc[i]['text'])
print('True label:', sample.loc[i]['hate_speech'], '\n')
print('ML input:', sample.loc[i]['clean_tweets_ml'])
print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
print("Transformer input:", sample.loc[i]['text'])
print("Transformer prediction:", sample.loc[i]['pred_trans'])

"""might often be misclassified. \\

Some errors could also be due to a too heavy pre-processing, which, even if careful, could sometimes change the meaning of the sentence. \\

Finally, it's interesting to note that the models often misclassify tweets/news containing hate speech but aimed at non-Muslims, Immigrates or Roma targets, whose ground truth is therefore $0$, as for example:
"""

sample = df_all_wrong[df_all_wrong['hate_speech']==0]
sample = sample.reset_index(drop=True)
i = 25

print(sample.loc[i]['text'])
print('True label:', sample.loc[i]['hate_speech'], '\n')
print('ML input:', sample.loc[i]['clean_tweets_ml'])
print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
print("Transformer input:", sample.loc[i]['text'])
print("Transformer prediction:", sample.loc[i]['pred_trans'])

"""This shows that they have learned to understand the presence of hate speech but do not distinguish with respect to the target the message is are aimed at.

#### News

Then, the same analysis procedure goes for test news.
"""

if 'spell_lemma' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=True, lemma=True, labels = 'hate_speech')
elif 'spell' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=True, lemma=False, labels = 'hate_speech')
elif 'lemma' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=False, lemma=True, labels = 'hate_speech')
else:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=False, lemma=False, labels = 'hate_speech')

if 'spell_lemma' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=True, lemma=True, labels = 'hate_speech')
elif 'spell' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=True, lemma=False, labels = 'hate_speech')
elif 'lemma' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=False, lemma=True, labels = 'hate_speech')
else:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=False, lemma=False, labels = 'hate_speech')

df_analysis = df_test_news.copy()

df_analysis['pred_rnn'] = pred_news_rnn
df_analysis['pred_ml'] = pred_news_ml
df_analysis['pred_trans'] = pred_news_trans
df_analysis['clean_news_ml'] = clean_news_ml
df_analysis['clean_news_rnn'] = clean_news_rnn

df_analysis = df_analysis.drop(['clean_text', 'stereotype'], axis=1)
df_analysis

df_all_wrong = df_analysis[(df_analysis['pred_rnn']==df_analysis['pred_ml']) & (df_analysis['pred_rnn']==df_analysis['pred_trans']) & (df_analysis['hate_speech']!=df_analysis['pred_ml'])]
df_all_wrong = df_all_wrong.reset_index(drop=True)
df_all_wrong

sample = df_all_wrong[df_all_wrong['hate_speech']==0]
sample = sample.reset_index(drop=True)

n = 0

for i in range(n, n+2):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['hate_speech'], '\n')
  print('ML input:', sample.loc[i]['clean_news_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_news_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

sample = df_all_wrong[df_all_wrong['hate_speech']==1]
sample = sample.reset_index(drop=True)

n = 23

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['hate_speech'], '\n')
  print('ML input:', sample.loc[i]['clean_news_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_news_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""It can be clearly seen how the news usually contains messages of hate expressed differently than tweets, where the message is more explicit and less nuanced. In addition, the news uses different language and style, on which the models have not been trained. Therefore, these might be possible causes of models poor performances on test news.

## Task B

### Best models selection
"""

def models_rank(model_type, scores_dict, models_dict):

  """
    Prints macro f1-scores for the given models.

    Parameters:
    - model_type (str): The type of the models.
    - scores_dict (dict): A dictionary containing the evaluation results for each model.
    - models_dict (dict): A dictionary containing the models.

    Returns:
    - f1_scores_macro_dict (dict): A dictionary containing the macro f1-scores of each model.
  """


  f1_scores_macro_dict = {}
  rows = [[f'{model_type} model','macro f1-score'],['','']]

  for model in models_dict.keys():
    f1_scores_macro_dict[model] = scores_dict[model]['macro avg']['f1-score']
    rows.append([model, scores_dict[model]['macro avg']['f1-score']])

  lens = []
  for col in zip(*rows):
      lens.append(max([len(str(v)) for v in col]))
  format = "  ".join(["{:<" + str(l) + "}" for l in lens])

  for row in rows:
      print(format.format(*row))

  return f1_scores_macro_dict

"""First, let's print the macro f1-score of all models on the test tweets."""

trans_f1_macro_dict = models_rank('Transformer', trans_scores_st_dict, trans_models_st_dict)
print()
ml_f1_macro_dict = models_rank('Classic ML', ml_scores_st_dict, ml_models_st_dict)
print()
rnn_f1_macro_dict = models_rank('RNN', rnn_scores_st_dict, rnn_models_st_dict)

"""Then, let's order them by macro f1-score for selecting only the best one per model type."""

sorted_f1_scores_rnn = sorted(rnn_f1_macro_dict, key=rnn_f1_macro_dict.get, reverse=True)
sorted_f1_scores_trans = sorted(trans_f1_macro_dict, key=trans_f1_macro_dict.get, reverse=True)
sorted_f1_scores_ml = sorted(ml_f1_macro_dict, key=ml_f1_macro_dict.get, reverse=True)

best_rnn_model = sorted_f1_scores_rnn[0]
best_trans_model = sorted_f1_scores_trans[0]
best_ml_model = sorted_f1_scores_ml[0]

print('According to the macro f1-score on the test set of tweets, the best Transformer model is:', best_trans_model)
print('According to the macro f1-score on the test set of tweets, the best classic ML model is:', best_ml_model)
print('According to the macro f1-score on the test set of tweets, the best RNN model is:', best_rnn_model)

"""### F1-score, precision and recall

In order to compare the best models results, it is useful to plot the macro f1-score, precision and recall obtained on both on the test tweets and the test news.
"""

pred_tweets_trans = trans_pred_tweets_st_dict[best_trans_model]
pred_tweets_ml =  ml_pred_tweets_st_dict[best_ml_model]
pred_tweets_rnn = rnn_pred_tweets_st_dict[best_rnn_model]

prfs_trans = prfs(df_test_tweets['stereotype'], pred_tweets_trans, average="macro")
prfs_ml = prfs(df_test_tweets['stereotype'], pred_tweets_ml, average="macro")
prfs_rnn = prfs(df_test_tweets['stereotype'], pred_tweets_rnn, average="macro")

fig, plots = plt.subplots(1,3, sharey=True, figsize=(18, 6))
fig.suptitle('Macro scores comparison - Test tweets')
plots[0].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[0], prfs_rnn[0], prfs_trans[0]], color=["darkred", "tab:orange", "navy"])
plots[0].set_title("Precision macro")
plots[1].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[1], prfs_rnn[1], prfs_trans[1]], color=["darkred", "tab:orange", "navy"])
plots[1].set_title("Recall macro")
plots[2].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[2], prfs_rnn[2], prfs_trans[2]], color=["darkred", "tab:orange", "navy"])
plots[2].set_title("Macro f1-score")

plt.show()

print()

pred_news_trans = trans_pred_news_st_dict[best_trans_model]
pred_news_ml =  ml_pred_news_st_dict[best_ml_model]
pred_news_rnn = rnn_pred_news_st_dict[best_rnn_model]

prfs_trans = prfs(df_test_news['stereotype'], pred_news_trans, average="macro")
prfs_ml = prfs(df_test_news['stereotype'], pred_news_ml, average="macro")
prfs_rnn = prfs(df_test_news['stereotype'], pred_news_rnn, average="macro")

fig, plots = plt.subplots(1,3, sharey=True, figsize=(18, 6))
fig.suptitle('Macro scores comparison - Test news')
plots[0].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[0], prfs_rnn[0], prfs_trans[0]], color=["darkred", "tab:orange", "navy"])
plots[0].set_title("Precision macro")
plots[1].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[1], prfs_rnn[1], prfs_trans[1]], color=["darkred", "tab:orange", "navy"])
plots[1].set_title("Recall macro")
plots[2].bar([best_ml_model, best_rnn_model, best_trans_model], [prfs_ml[2], prfs_rnn[2], prfs_trans[2]], color=["darkred", "tab:orange", "navy"])
plots[2].set_title("Macro f1-score")

plt.show()

"""Results of the three best models are again fully comparable, with no significant difference between precision, recall and f1-score of models for both test tweets and news."""

prfs_1 = prfs(df_test_tweets['stereotype'], pred_tweets_trans, labels=[0,1], average=None, zero_division=0)
prfs_2 = prfs(df_test_tweets['stereotype'], pred_tweets_ml, labels=[0,1], average=None, zero_division=0)
prfs_3 = prfs(df_test_tweets['stereotype'], pred_tweets_rnn, labels=[0,1], average=None, zero_division=0)

prfs_4 = prfs(df_test_news['stereotype'], pred_news_trans, labels=[0,1], average=None, zero_division=0)
prfs_5 = prfs(df_test_news['stereotype'], pred_news_ml, labels=[0,1], average=None, zero_division=0)
prfs_6 = prfs(df_test_news['stereotype'], pred_news_rnn, labels=[0,1], average=None, zero_division=0)

width=0.15

ids = np.arange(0,2)
x_labels = [0, 1]

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('Precision')
ax[0].bar(ids-5/4*width, prfs_1[0], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[0], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[0], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[0], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[0], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[0], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')


plt.show()
print()

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('Recall')
ax[0].bar(ids-5/4*width, prfs_1[1], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[1], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[1], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[1], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[1], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[1], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')

plt.show()
print()

fig, ax = plt.subplots(1,2, sharey=True, figsize=(15,6))
fig.suptitle('F1-score')
ax[0].bar(ids-5/4*width, prfs_1[2], width, label=best_trans_model, color="navy")
ax[0].bar(ids+5/4*width, prfs_2[2], width, label=best_ml_model, color="darkred")
ax[0].bar(ids, prfs_3[2], width, label=best_rnn_model, color="tab:orange")
ax[0].set_title("Test tweets")
ax[1].bar(ids-5/4*width, prfs_4[2], width, label=best_trans_model, color="navy")
ax[1].bar(ids+5/4*width, prfs_5[2], width, label=best_ml_model, color="darkred")
ax[1].bar(ids, prfs_6[2], width, label=best_rnn_model, color="tab:orange")
ax[1].set_title("Test news")
ax[0].set_ylim([0,1.18])
ax[0].set_xlim([-0.5,1.5])
ax[0].set_xticks(ids)
ax[0].set_xticklabels(x_labels)
ax[1].set_ylim([0,1.18])
ax[1].set_xlim([-0.5,1.5])
ax[1].set_xticks(ids)
ax[1].set_xticklabels(x_labels)
ax[0].legend(loc='upper center')
ax[1].legend(loc='upper center')

plt.show()

"""Again, on test tweets each model shows consistency between the per-class precision, recall and f1-score and the macro ones. Only UmBERTo achieves on test tweets an high precision on class $0$, at the expense of a bad recall on the same class, while reaching an high recall on class $1$, but with a correspondent lower precision. \\


Also, models per-class recall and f1-score on test news are always higher on the class $0$ than on the class $1$, with a low precision on both classes. Likewise the hate speech detection task, this may be due to the slightly imbalanced nature of the training data towards the $0$ class, but the difference in the way the stereotype is expressed between the training tweets and the test news is what affects it most.

### Confusion matrices

As a further analysis, let's visualize the confusion matrices for all the models, both on tweets and news test sets.
"""

fig, axn = plt.subplots(1,3, sharey=True, figsize=(20, 5))

fig.suptitle('Test tweets confusion matrices')

c1 = confusion_matrix(df_test_tweets['stereotype'], pred_tweets_ml, normalize='true', labels=[0,1])
h1 = sns.heatmap(c1, annot = True, fmt = '.2f', ax = axn.flat[0], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h1.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[0].set_title(best_ml_model, fontsize=12)

c2 = confusion_matrix(df_test_tweets['stereotype'], pred_tweets_rnn, normalize='true', labels=[0,1])
h2 = sns.heatmap(c2, annot = True, fmt = '.2f', ax = axn.flat[1], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h2.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[1].set_title(best_rnn_model, fontsize=12)

c3 = confusion_matrix(df_test_tweets['stereotype'], pred_tweets_trans, normalize='true', labels=[0,1])
h3 = sns.heatmap(c3, annot = True, fmt = '.2f', ax = axn.flat[2], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h3.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[2].set_title(best_trans_model, fontsize=12)

plt.show()


print()


fig, axn = plt.subplots(1,3, sharey=True, figsize=(20, 5))

fig.suptitle('Test news confusion matrices')

c1 = confusion_matrix(df_test_news['stereotype'], pred_news_ml, normalize='true', labels=[0,1])
h1 = sns.heatmap(c1, annot = True, fmt = '.2f', ax = axn.flat[0], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h1.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[0].set_title(best_ml_model, fontsize=12)


c2 = confusion_matrix(df_test_news['stereotype'], pred_news_rnn, normalize='true', labels=[0,1])
h2 = sns.heatmap(c2, annot = True, fmt = '.2f', ax = axn.flat[1], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h2.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[1].set_title(best_rnn_model, fontsize=12)

c3 = confusion_matrix(df_test_news['stereotype'], pred_news_trans, normalize='true', labels=[0,1])
h3 = sns.heatmap(c3, annot = True, fmt = '.2f', ax = axn.flat[2], cmap='Blues', xticklabels = [0,1], yticklabels = [0,1])
h3.set(xlabel='Predicted Labels', ylabel='True Labels')
axn.flat[2].set_title(best_trans_model, fontsize=12)

plt.show()

"""As for the hate speech detection task, from those plots it is even clearer that models really struggle in identifying correctly news containing stereotype. \\
Also UmBERTo behaviour is more evident by looking at its confusion matrix.

### Specific errors analysis

Let's now have a deeper look into samples which are completely misclassified by all models.

First, can be useful to print some statistics about the most frequent words in all the sets.
"""

def most_frequent_words(df, n_print, split):

  """
  After applying basic pre-processing and removing stopwords, it finds the most frequent words in the given DataFrame and prints the top `n_print` words.

  Parameters:
  - df (pandas.DataFrame): The DataFrame containing the text data.
  - n_print (int): The number of most frequent words to be printed.
  - split (str): The name of the split to be included in the output.

  Returns:
  - list: A list of tuples, where each tuple contains a word and its frequency, sorted in descending order of frequency.

  """

  wordcount = {}

  prova = df['text'].copy()
  prova = cleaning_text(prova)
  prova = remove_stopwords(prova)

  for p in prova:
    for word in p.lower().split():
        if word != 'non':
          if word not in wordcount:
              wordcount[word] = 1
          else:
              wordcount[word] += 1

  print("\n{} SET".format(split))
  word_counter = collections.Counter(wordcount)
  for word, count in word_counter.most_common(n_print):
      print(word, ": ", count)
  print()

  return word_counter.most_common(n_print)

df_train_st_1 = df_train[df_train['stereotype']==1].copy()
df_val_st_1 = df_val[df_val['stereotype']==1].copy()
df_test_tweets_st_1 = df_test_tweets[df_test_tweets['stereotype']==1].copy()
df_test_news_st_1 = df_test_news[df_test_news['stereotype']==1].copy()

n_print = 10
print(f"Most {n_print} common words among all the tweets/news marked as stereotype, for each set:")

train_wc_st_1 = most_frequent_words(df_train_st_1, n_print, 'TRAINING')
val_wc_st_1 = most_frequent_words(df_val_st_1, n_print, 'VALIDATION')
test_tweets_wc_st_1 = most_frequent_words(df_test_tweets_st_1, n_print, 'TWEETS TEST')
test_news_wc_st_1 = most_frequent_words(df_test_news_st_1, n_print, 'NEWS TEST')

df_train_st_0 = df_train[df_train['stereotype']==0].copy()
df_val_st_0 = df_val[df_val['stereotype']==0].copy()
df_test_tweets_st_0 = df_test_tweets[df_test_tweets['stereotype']==0].copy()
df_test_news_st_0 = df_test_news[df_test_news['stereotype']==0].copy()

n_print = 10
print(f"Most {n_print} common words among all the tweets/news not marked as stereotype, for each set:")

train_wc_st_0 = most_frequent_words(df_train_st_0, n_print, 'TRAINING')
val_wc_st_0 = most_frequent_words(df_val_st_0, n_print, 'VALIDATION')
test_tweets_wc_st_0 = most_frequent_words(df_test_tweets_st_0, n_print, 'TWEETS TEST')
test_news_wc_st_0 = most_frequent_words(df_test_news_st_0, n_print, 'NEWS TEST')

train_st_1 = [train_wc_st_1[i][0] for i in range(0, len(train_wc_st_1))]
train_st_0 = [train_wc_st_0[i][0] for i in range(0, len(train_wc_st_0))]

"""The most informative words are those that appear frequently in train tweets belonging to one class, but not in those of the other."""

print("Words that are frequent in stereotype train tweets but not in non-stereotype ones:")
words_st = [w for w in train_st_1 if w not in train_st_0]
words_st

print("Words that are frequent in non-stereotype train tweets but not in stereotype ones:")
words_not_st = [w for w in train_st_0 if w not in train_st_1]
words_not_st

"""#### Tweets

For the purpose of analysis, it is needed to retrieve best models predictions.

Also, it might be useful to visualize which kind of input the models have been fed with. So, let's also retrieve the pre-processed inputs, according to the models under analysis.
"""

if 'spell_lemma' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=True, labels = 'stereotype')
elif 'spell' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=False, labels = 'stereotype')
elif 'lemma' in best_ml_model:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=True, labels = 'stereotype')
else:
  clean_tweets_ml, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=False, labels = 'stereotype')


if 'spell_lemma' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=True, labels = 'stereotype')
elif 'spell' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=True, lemma=False, labels = 'stereotype')
elif 'lemma' in best_rnn_model:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=True, labels = 'stereotype')
else:
  clean_tweets_rnn, _ = preprocessing_function(df_test_tweets, spell_check=False, lemma=False, labels = 'stereotype')

"""For conveniency reasons, all predictions and preprocessed tweets are collected in a single dataframe, which contains also original tweets and true Stereotype labels."""

df_analysis = df_test_tweets.copy()

df_analysis['pred_rnn'] = pred_tweets_rnn
df_analysis['pred_ml'] = pred_tweets_ml
df_analysis['pred_trans'] = pred_tweets_trans
df_analysis['clean_tweets_ml'] = clean_tweets_ml
df_analysis['clean_tweets_rnn'] = clean_tweets_rnn

df_analysis = df_analysis.drop(['hate_speech'], axis=1)
df_analysis

"""Now let's select only those samples that are misclassified by all models under analysis."""

df_all_wrong = df_analysis[(df_analysis['pred_rnn']==df_analysis['pred_ml']) & (df_analysis['pred_rnn']==df_analysis['pred_trans']) & (df_analysis['stereotype']!=df_analysis['pred_ml'])]
df_all_wrong = df_all_wrong.reset_index(drop=True)
df_all_wrong

"""Then only print some of those that do not contain stereotypes (whose Stereotype label is 0)."""

sample = df_all_wrong[df_all_wrong['stereotype']==0]
sample = sample.reset_index(drop=True)

n = 23

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['stereotype'], '\n')
  print('ML input:', sample.loc[i]['clean_tweets_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""Now instead only print some of those that actually contain stereotypes (whose Stereotype label is 1)."""

sample = df_all_wrong[df_all_wrong['stereotype']==1]
sample = sample.reset_index(drop=True)
n = 20

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['stereotype'], '\n')
  print('ML input:', sample.loc[i]['clean_tweets_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""Looking deeper into misclassified samples from all models, some interesting possible causes of error have emerged.

Texts that contain words that are very frequent in one class but rare in the other may also challenge the classification. Indeed, these words during training can create a misclassification bias toward the class in which the word is more present, letting the model to learn a strong (but sometimes incorrect) relationship between those words and the class. For example, the word "*clandestini*" is one of the 10 most frequent words among all the training tweets marked as hate speech, but it's not frequent among the non-hate speech ones. Thus, when it appears in a text, models might misclassified it as stereotype, regardless of everything else. An example of this kind of behaviour might be:
"""

sample = df_all_wrong[df_all_wrong['stereotype']==0]
sample = sample.reset_index(drop=True)
i = 24

print(sample.loc[i]['text'])
print('True label:', sample.loc[i]['stereotype'], '\n')
print('ML input:', sample.loc[i]['clean_tweets_ml'])
print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
print('RNN input:', sample.loc[i]['clean_tweets_rnn'])
print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
print("Transformer input:", sample.loc[i]['text'])
print("Transformer prediction:", sample.loc[i]['pred_trans'])

"""#### News

Then, the same analysis procedure goes for test news.
"""

if 'spell_lemma' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=True, lemma=True, labels = 'stereotype')
elif 'spell' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=True, lemma=False, labels = 'stereotype')
elif 'lemma' in best_ml_model:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=False, lemma=True, labels = 'stereotype')
else:
  clean_news_ml, _ = preprocessing_function(df_test_news, spell_check=False, lemma=False, labels = 'stereotype')


if 'spell_lemma' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=True, lemma=True, labels = 'stereotype')
elif 'spell' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=True, lemma=False, labels = 'stereotype')
elif 'lemma' in best_rnn_model:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=False, lemma=True, labels = 'stereotype')
else:
  clean_news_rnn, _ = preprocessing_function(df_test_news, spell_check=False, lemma=False, labels = 'stereotype')

df_analysis = df_test_news.copy()

df_analysis['pred_rnn'] = pred_news_rnn
df_analysis['pred_ml'] = pred_news_ml
df_analysis['pred_trans'] = pred_news_trans
df_analysis['clean_news_ml'] = clean_news_ml
df_analysis['clean_news_rnn'] = clean_news_rnn

df_analysis = df_analysis.drop(['hate_speech'], axis=1)
df_analysis

df_all_wrong = df_analysis[(df_analysis['pred_rnn']==df_analysis['pred_ml']) & (df_analysis['pred_rnn']==df_analysis['pred_trans']) & (df_analysis['stereotype']!=df_analysis['pred_ml'])]
df_all_wrong = df_all_wrong.reset_index(drop=True)
df_all_wrong

sample = df_all_wrong[df_all_wrong['stereotype']==0]
sample = sample.reset_index(drop=True)

n = 0

for i in range(n, n+2):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['stereotype'], '\n')
  print('ML input:', sample.loc[i]['clean_news_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_news_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

sample = df_all_wrong[df_all_wrong['stereotype']==1]
sample = sample.reset_index(drop=True)

n = 30

for i in range(n, n+5):
  print()
  print(sample.loc[i]['text'])
  print('True label:', sample.loc[i]['stereotype'], '\n')
  print('ML input:', sample.loc[i]['clean_news_ml'])
  print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
  print('RNN input:', sample.loc[i]['clean_news_rnn'])
  print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
  print("Transformer input:", sample.loc[i]['text'])
  print("Transformer prediction:", sample.loc[i]['pred_trans'])
  print()
  print('-'*500)

"""Again, it can be clearly noticed that the different language style used in tweets and news really challenges models classification.

Some peculiar misclassified samples are those probably due to the presence of sarcastic or ironic messages in it. In fact, it is difficult for models to detect stereotype in texts where stereotype is expressed in an implicit form, hidden behind sarcasm, such as:
"""

sample = df_all_wrong[df_all_wrong['stereotype']==1]
sample = sample.reset_index(drop=True)
i = 34

print(sample.loc[i]['text'])
print('True label:', sample.loc[i]['stereotype'], '\n')
print('ML input:', sample.loc[i]['clean_news_ml'])
print("ML prediction:", sample.loc[i]['pred_ml'], '\n')
print('RNN input:', sample.loc[i]['clean_news_rnn'])
print("RNN prediction:", sample.loc[i]['pred_rnn'], '\n')
print("Transformer input:", sample.loc[i]['text'])
print("Transformer prediction:", sample.loc[i]['pred_trans'])